# api/tasks.py
import requests
import json
import logging
from datetime import datetime, timedelta, date, timezone as dt_timezone # Import timezone t·ª´ datetime
from django.conf import settings
from django.db import transaction
from django.utils import timezone # S·ª≠ d·ª•ng timezone c·ªßa Django
from concurrent.futures import ThreadPoolExecutor, as_completed # Import ThreadPoolExecutor

from .models import Location, WeatherData, ExtremeEvent

# Thi·∫øt l·∫≠p logger ri√™ng cho file tasks
# Level INFO s·∫Ω ghi l·∫°i c√°c b∆∞·ªõc ch√≠nh, DEBUG s·∫Ω ghi chi ti·∫øt h∆°n
logger = logging.getLogger(__name__)

# --- H√ÄM TI·ªÜN √çCH CHO TASKS ---

def call_weather_api_from_task(endpoint, params):
    """
    H√†m g·ªçi API WeatherAPI d√†nh ri√™ng cho tasks, x·ª≠ l√Ω l·ªói chi ti·∫øt h∆°n.
    Tr·∫£ v·ªÅ tuple: (data, error_message). data l√† None n·∫øu c√≥ l·ªói.
    """
    if not settings.WEATHER_API_KEY:
        logger.error("WEATHER_API_KEY is not configured.")
        return None, "Weather API Key missing"

    params['key'] = settings.WEATHER_API_KEY
    params['lang'] = 'vi'
    full_url = f"{settings.BASE_WEATHER_URL}/{endpoint}.json"

    try:
        # TƒÉng timeout l√™n 30 gi√¢y cho c√°c cu·ªôc g·ªçi API m·∫°ng
        response = requests.get(full_url, params=params, timeout=30)
        response.raise_for_status() # N√©m l·ªói HTTPError cho status >= 400
        return response.json(), None # Tr·∫£ v·ªÅ d·ªØ li·ªáu JSON n·∫øu th√†nh c√¥ng
    except requests.exceptions.Timeout:
        logger.warning(f"Timeout calling WeatherAPI endpoint: {endpoint} for location: {params.get('q')}")
        return None, "API Timeout"
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code
        try:
            # C·ªë g·∫Øng l·∫•y l·ªói JSON n·∫øu c√≥
            error_data = e.response.json()
        except json.JSONDecodeError:
            # N·∫øu kh√¥ng ph·∫£i JSON, l·∫•y text th√¥
            error_data = {'message': e.response.text[:500]} # Gi·ªõi h·∫°n ƒë·ªô d√†i l·ªói
        logger.error(f"HTTP Error calling WeatherAPI ({endpoint}) for {params.get('q')}: {status_code} - {error_data}")
        return None, f"API HTTP Error: {status_code}"
    except requests.exceptions.RequestException as e:
        logger.error(f"General Error calling WeatherAPI ({endpoint}) for {params.get('q')}: {e}")
        return None, f"API Request Error: {e}"
    except Exception as e:
        # Ghi l·∫°i l·ªói kh√¥ng mong mu·ªën k√®m traceback
        logger.error(f"Unexpected error in call_weather_api_from_task: {e}", exc_info=True)
        return None, f"Unexpected Error: {e}"


def call_local_ai_for_analysis(time_series_data):
    """
    H√†m g·ªçi API Ollama c·ª•c b·ªô ƒë·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu th·ªùi ti·∫øt.
    S·ª≠ d·ª•ng prompt "Chuy√™n gia Th·∫≠n tr·ªçng" v√† tr·∫£ v·ªÅ m·∫£ng c·∫£nh b√°o.
    ƒê√£ c·∫≠p nh·∫≠t ƒë·ªÉ x·ª≠ l√Ω response linh ho·∫°t (list ho·∫∑c dict ƒë∆°n).
    """
    prompt = f"""
        **VAI TR√í:**
        B·∫°n l√† m·ªôt chuy√™n gia kh√≠ t∆∞·ª£ng th·ªßy vƒÉn th·∫≠n tr·ªçng.

        **QUY T·∫ÆC V√ÄNG: H√ÉY HO√ÄI NGHI.** C√¢u tr·∫£ l·ªùi m·∫∑c ƒë·ªãnh l√† m·ªôt m·∫£ng r·ªóng [].

        **D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO:**
        Chu·ªói d·ªØ li·ªáu th·ªùi ti·∫øt 14 ng√†y (l·ªãch s·ª≠ + d·ª± b√°o):
        {json.dumps(time_series_data, indent=2, default=str)}

        **C√ÅC NG∆Ø·ª†NG K√çCH HO·∫†T C·∫¢NH B√ÅO (Ch·ªâ b√°o c√°o n·∫øu v∆∞·ª£t ng∆∞·ª°ng):**
        - Ch√°y r·ª´ng (INFRASTRUCTURE - HIGH/CRITICAL): Nhi·ªát ƒë·ªô (avgtemp_c) > 37¬∞C trong √çT NH·∫§T 3 ng√†y V√Ä ƒë·ªô ·∫©m (avghumidity) < 40%.
        - S·ªëc nhi·ªát (PUBLIC_HEALTH - HIGH): Nhi·ªát ƒë·ªô (avgtemp_c) > 38¬∞C V√Ä UV > 10 trong √çT NH·∫§T 2 ng√†y.
        - S√¢u b·ªánh (AGRICULTURE - MEDIUM): ƒê·ªô ·∫©m (avghumidity) > 90% trong √çT NH·∫§T 4 ng√†y V√Ä nhi·ªát ƒë·ªô (avgtemp_c) > 25¬∞C.

        **Y√äU C·∫¶U ƒê·∫¶U RA:**
        Ch·ªâ tr·∫£ l·ªùi b·∫±ng m·ªôt M·∫¢NG (array) c√°c ƒë·ªëi t∆∞·ª£ng JSON. N·∫øu kh√¥ng c√≥ r·ªßi ro, tr·∫£ v·ªÅ [].
        C·∫•u tr√∫c c·ªßa m·ªói ƒë·ªëi t∆∞·ª£ng:
        {{
          "severity": "M·ª©c ƒë·ªô ('MEDIUM', 'HIGH', 'CRITICAL')",
          "impact_field": "Lƒ©nh v·ª±c ('AGRICULTURE', 'INFRASTRUCTURE', 'PUBLIC_HEALTH')",
          "forecast_details_vi": "M√¥ t·∫£ r·ªßi ro v√† tr√≠ch d·∫´n S·ªê LI·ªÜU b·∫±ng ch·ª©ng.",
          "actionable_advice_vi": "ƒê∆∞a ra m·ªôt c√¢u KHUY·∫æN NGH·ªä h√†nh ƒë·ªông c·ª• th·ªÉ."
        }}
    """
    try:
        # Log nh·∫π nh√†ng h∆°n khi g·ªçi AI
        logger.debug("[LOCAL AI] Sending analysis request to Ollama...")
        # TƒÉng timeout l√™n 5 ph√∫t (300 gi√¢y) v√¨ AI c√≥ th·ªÉ c·∫ßn nhi·ªÅu th·ªùi gian
        response = requests.post(settings.OLLAMA_API_URL, json={
            "model": "gemma3",
            "prompt": prompt,
            "format": "json",
            "stream": False
        }, timeout=300)
        response.raise_for_status()

        response_data = response.json()
        # Ollama tr·∫£ v·ªÅ JSON string trong tr∆∞·ªùng 'response'
        if 'response' in response_data:
            try:
                # Parse JSON string t·ª´ response c·ªßa Ollama
                raw_result = json.loads(response_data['response'])

                # --- X·ª≠ l√Ω Response Linh ho·∫°t ---
                if isinstance(raw_result, list):
                    # N·∫øu AI tr·∫£ v·ªÅ ƒë√∫ng l√† m·ªôt list (k·ªÉ c·∫£ list r·ªóng []), d√πng n√≥ lu√¥n
                    return raw_result, None
                elif isinstance(raw_result, dict) and raw_result.get('severity', 'NONE').upper() != 'NONE':
                    # N·∫øu AI tr·∫£ v·ªÅ m·ªôt object v√† c√≥ severity kh√°c NONE => c·∫£nh b√°o ƒë∆°n l·∫ª
                    logger.warning(f"[LOCAL AI] Ollama returned a single object, wrapping in list: {raw_result}")
                    return [raw_result], None # G√≥i v√†o list
                elif isinstance(raw_result, dict) and not raw_result:
                    # N·∫øu AI tr·∫£ v·ªÅ object r·ªóng {}
                    logger.debug("[LOCAL AI] Ollama returned an empty object, treating as no alerts.")
                    return [], None # Coi nh∆∞ kh√¥ng c√≥ c·∫£nh b√°o
                else:
                    # C√°c tr∆∞·ªùng h·ª£p kh√°c (object c√≥ severity NONE, ho·∫∑c kh√¥ng ph·∫£i list/dict)
                    logger.warning(f"[LOCAL AI] Ollama response was not a valid list/object: {raw_result}")
                    return [], "AI response invalid structure"
                # --- K·∫øt th√∫c X·ª≠ l√Ω Linh ho·∫°t ---

            except json.JSONDecodeError as e:
                logger.error(f"[LOCAL AI] Error parsing JSON from Ollama response: {e}")
                logger.error(f"Ollama raw response string: {response_data.get('response', 'N/A')}")
                return [], "AI Response Parsing Error"
        else:
             logger.warning(f"[LOCAL AI] 'response' field missing in Ollama output: {response_data}")
             return [], "AI response field missing"

    except requests.exceptions.Timeout:
        logger.error("[LOCAL AI] Timeout calling local Ollama API (waited 300 seconds).")
        return [], "AI Timeout"
    except requests.exceptions.RequestException as e:
        logger.error(f"[LOCAL AI] Error calling Ollama API: {e}")
        logger.info("üí° Tip: Ensure Ollama is running and the 'gemma3' model is downloaded ('ollama run gemma3').")
        return [], f"AI Connection Error: {e}"
    except Exception as e:
        logger.error(f"[LOCAL AI] Unexpected error during AI analysis: {e}", exc_info=True)
        return [], f"Unexpected AI Error: {e}"

# --- C√ÅC H√ÄM T√ÅC V·ª§ N·ªÄN (CRON JOBS) ---

@transaction.atomic # ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c thao t√°c CSDL trong h√†m n√†y th√†nh c√¥ng ho·∫∑c th·∫•t b·∫°i c√πng nhau
def trigger_data_ingestion():
    """ 
    T√°c v·ª• thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ v√† d·ª± b√°o (Cron job ch·∫°y h√†ng lo·∫°t) 
    H√†m n√†y gi·ªù ch·ªâ g·ªçi h√†m con 'ingest_data_for_single_location'.
    """
    logger.info("--- [TASK START] Running Full Data Ingestion ---")
    active_locations = Location.objects.filter(is_active=True)
    if not active_locations.exists():
        logger.info("[DATA INGESTION] No active locations.")
        return {'success': True, 'message': 'No active locations.'}

    logger.info(f"[DATA INGESTION] Found {active_locations.count()} active location(s).")

    total_success = 0
    total_fail = 0

    # V√≤ng l·∫∑p n√†y gi·ªù ƒë√£ s·∫°ch v√† ƒë∆°n gi·∫£n h∆°n r·∫•t nhi·ªÅu
    for loc in active_locations:
        try:
            # G·ªçi h√†m con cho t·ª´ng ƒë·ªãa ƒëi·ªÉm
            success = ingest_data_for_single_location(loc.location_id) 
            if success:
                total_success += 1
            else:
                total_fail += 1
                logger.warning(f"[DATA INGESTION] Failed to ingest data for loc {loc.location_id} during cron job.")
        except Exception as e:
            # L·ªói nghi√™m tr·ªçng khi ch·∫°y h√†m con
            logger.error(f"[DATA INGESTION] Critical error processing loc {loc.location_id}: {e}", exc_info=True)
            total_fail += 1

    errors_occurred = total_fail > 0
    logger.info(f"--- [TASK FINISH] Data Ingestion completed. Succeeded for {total_success} locations. Failed for {total_fail} locations. ---")
    return {'success': not errors_occurred, 'message': f'Data Ingestion completed. Success: {total_success}, Fail: {total_fail}.'}

# --- H√ÄM CON ƒê·ªÇ X·ª¨ L√ù M·ªòT TH√ÄNH PH·ªê (ƒê·ªäNH NGHƒ®A TR∆Ø·ªöC) ---
def analyze_single_location(loc):
    """ L·∫•y d·ªØ li·ªáu, g·ªçi AI v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ cho m·ªôt th√†nh ph·ªë duy nh·∫•t """
    logger.debug(f"[LLM ANALYSIS] Analyzing location: {loc.name_en} (ID: {loc.location_id})")
    # L·∫•y 14 ng√†y d·ªØ li·ªáu g·∫ßn nh·∫•t, s·ª≠ d·ª•ng select_related ƒë·ªÉ t·ªëi ∆∞u
    time_series_qs = WeatherData.objects.select_related('location').filter(location=loc).order_by('-record_time')[:14]

    if len(time_series_qs) < 14:
        logger.warning(f"[LLM ANALYSIS] Not enough data for {loc.name_en} ({len(time_series_qs)}/14). Skipping.")
        # Tr·∫£ v·ªÅ l·ªói ƒë·ªÉ h√†m cha bi·∫øt t√°c v·ª• con th·∫•t b·∫°i
        return loc, [], "Not enough data"

    # Chuy·ªÉn ƒë·ªïi v√† s·∫Øp x·∫øp c·∫©n th·∫≠n
    try:
        time_series_data = sorted(
            list(time_series_qs.values('record_time', 'temp_c', 'humidity', 'wind_kph', 'data_type')),
            key=lambda x: x['record_time'] # S·∫Øp x·∫øp theo record_time
        )
    except Exception as e:
        logger.error(f"Error preparing time series data for {loc.name_en}: {e}", exc_info=True)
        return loc, [], f"Data preparation error: {e}"

    # G·ªçi AI
    alert_results, ai_err = call_local_ai_for_analysis(time_series_data)
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£, bao g·ªìm c·∫£ l·ªói AI n·∫øu c√≥
    return loc, alert_results, ai_err


# --- H√ÄM PH√ÇN T√çCH AI CH√çNH (X·ª¨ L√ù ƒê·ªíNG TH·ªúI) ---
@transaction.atomic # ƒê·∫£m b·∫£o l∆∞u CSDL an to√†n khi ch·∫°y song song
def trigger_llm_analysis():
    """ T√°c v·ª• ph√¢n t√≠ch AI - Ch·∫°y ƒë·ªìng th·ªùi cho nhi·ªÅu th√†nh ph·ªë """
    logger.info("--- [TASK START] Running CONCURRENT Local LLM Analysis ---")
    active_locations = Location.objects.filter(is_active=True)
    if not active_locations.exists():
        logger.info("[LLM ANALYSIS] No active locations.")
        return {'success': True, 'message': 'No active locations.'}

    logger.info(f"[LLM ANALYSIS] Found {active_locations.count()} active location(s) for concurrent analysis.")
    alerts_created_count = 0
    errors_occurred_ai = False # C·ªù l·ªói ri√™ng cho vi·ªác g·ªçi/parse AI response
    errors_occurred_db = False # C·ªù l·ªói ri√™ng cho vi·ªác l∆∞u CSDL
    errors_occurred_data = False # C·ªù l·ªói ri√™ng cho vi·ªác chu·∫©n b·ªã data

    # S·ª≠ d·ª•ng ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=3) as executor: # Gi·ªõi h·∫°n s·ªë lu·ªìng
        # Submit tasks
        future_to_loc_obj = {executor.submit(analyze_single_location, loc_obj): loc_obj for loc_obj in active_locations}

        # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
        for future in as_completed(future_to_loc_obj):
            loc_obj_from_future = future_to_loc_obj[future]
            try:
                analyzed_loc_obj, alert_results, task_err = future.result()

                # Ki·ªÉm tra l·ªói tr·∫£ v·ªÅ t·ª´ t√°c v·ª• con
                if task_err and task_err != "Not enough data":
                    errors_occurred_ai = True
                    logger.error(f"AI analysis task failed for {analyzed_loc_obj.name_en}: {task_err}")
                    continue
                elif task_err == "Not enough data":
                    errors_occurred_data = True # Ghi nh·∫≠n l·ªói thi·∫øu data
                    continue

                # L∆∞u k·∫øt qu·∫£ c·∫£nh b√°o (n·∫øu c√≥)
                if alert_results:
                    logger.info(f"[LLM ANALYSIS] Storing {len(alert_results)} alert(s) for {analyzed_loc_obj.name_en}...")
                    for alert in alert_results:
                        required_keys = ['severity', 'impact_field', 'forecast_details_vi', 'actionable_advice_vi']
                        if isinstance(alert, dict) and all(k in alert and isinstance(alert[k], str) and alert[k] for k in required_keys):
                            try:
                                ExtremeEvent.objects.create(
                                    location=analyzed_loc_obj,
                                    severity=alert['severity'],
                                    impact_field=alert['impact_field'],
                                    forecast_details_vi=alert['forecast_details_vi'],
                                    actionable_advice_vi=alert['actionable_advice_vi'],
                                    raw_llm_json=alert
                                )
                                alerts_created_count += 1
                            except Exception as db_exc:
                                logger.error(f"Error saving alert for {analyzed_loc_obj.name_en}: {db_exc}", exc_info=True)
                                errors_occurred_db = True
                        else:
                            logger.warning(f"[LLM ANALYSIS] Invalid alert structure for {analyzed_loc_obj.name_en}: {alert}")
                            # Kh√¥ng b·∫≠t c·ªù l·ªói AI ·ªü ƒë√¢y n·∫øu call_local_ai_for_analysis ƒë√£ x·ª≠ l√Ω
            except Exception as exc:
                logger.error(f"Error processing result for location {loc_obj_from_future.name_en}: {exc}", exc_info=True)
                errors_occurred_ai = True

    any_critical_errors = errors_occurred_ai or errors_occurred_db
    logger.info(f"--- [TASK FINISH] CONCURRENT LLM Analysis completed. Created {alerts_created_count} alerts. Critical errors: {any_critical_errors} (AI: {errors_occurred_ai}, DB: {errors_occurred_db}, Data: {errors_occurred_data}) ---")
    return {'success': not any_critical_errors, 'message': f'Concurrent analysis completed. Created {alerts_created_count} alerts.'}

def ingest_data_for_single_location(location_id):
    """ T√°c v·ª• thu th·∫≠p d·ªØ li·ªáu t·ª©c th√¨ cho M·ªòT ƒë·ªãa ƒëi·ªÉm m·ªõi. """
    try:
        loc = Location.objects.get(location_id=location_id)
        logger.info(f"[INSTANT INGEST] Running for new location: {loc.name_en} (ID: {loc.location_id})")
    except Location.DoesNotExist:
        logger.error(f"[INSTANT INGEST] Location ID {location_id} not found.")
        return False

    # L·∫•y 7 ng√†y l·ªãch s·ª≠ v√† 7 ng√†y d·ª± b√°o (gi·ªëng h·ªát logic trong h√†m cron)
    today = timezone.now().date()
    end_date_hist = today - timedelta(days=1)
    start_date_hist = end_date_hist - timedelta(days=6)
    dt_str = start_date_hist.strftime('%Y-%m-%d')
    end_dt_str = end_date_hist.strftime('%Y-%m-%d')

    all_records_to_insert = []
    errors_occurred = False

    # --- L·∫•y l·ªãch s·ª≠ ---
    logger.debug(f"[INSTANT INGEST] Fetching history for {loc.name_en}")
    hist_data, hist_err = call_weather_api_from_task('history', {'q': loc.name_en, 'dt': dt_str, 'end_dt': end_dt_str})
    if hist_data and 'forecast' in hist_data and 'forecastday' in hist_data['forecast']:
        for day in hist_data['forecast']['forecastday']:
            try:
                record_dt_naive = datetime.strptime(day['date'], '%Y-%m-%d')
                record_dt_aware = timezone.make_aware(record_dt_naive, dt_timezone.utc)
                all_records_to_insert.append(WeatherData(
                    location=loc, record_time=record_dt_aware, data_type='HISTORY',
                    temp_c=day['day'].get('avgtemp_c'), humidity=day['day'].get('avghumidity'),
                    uv_index=day['day'].get('uv'), wind_kph=day['day'].get('maxwind_kph'), raw_json=day
                ))
            except (ValueError, KeyError, TypeError) as e:
                logger.warning(f"[INSTANT INGEST] Skipping invalid history record for {loc.name_en} on {day.get('date')}: {e}")
    elif hist_err:
        errors_occurred = True
        logger.error(f"[INSTANT INGEST] Failed to fetch history for {loc.name_en}: {hist_err}")

    # --- L·∫•y d·ª± b√°o ---
    logger.debug(f"[INSTANT INGEST] Fetching 7-day forecast for {loc.name_en}")
    fc_data, fc_err = call_weather_api_from_task('forecast', {'q': loc.name_en, 'days': 7})
    if fc_data and 'forecast' in fc_data and 'forecastday' in fc_data['forecast']:
        for day in fc_data['forecast']['forecastday']:
             try:
                record_dt_naive = datetime.strptime(day['date'], '%Y-%m-%d')
                record_dt_aware = timezone.make_aware(record_dt_naive, dt_timezone.utc)
                all_records_to_insert.append(WeatherData(
                    location=loc, record_time=record_dt_aware, data_type='FORECAST',
                    temp_c=day['day'].get('avgtemp_c'), humidity=day['day'].get('avghumidity'),
                    uv_index=day['day'].get('uv'), wind_kph=day['day'].get('maxwind_kph'), raw_json=day
                ))
             except (ValueError, KeyError, TypeError) as e:
                logger.warning(f"[INSTANT INGEST] Skipping invalid forecast record for {loc.name_en} on {day.get('date')}: {e}")
    elif fc_err:
        errors_occurred = True
        logger.error(f"[INSTANT INGEST] Failed to fetch forecast for {loc.name_en}: {fc_err}")

    # --- Bulk insert ---
    if all_records_to_insert:
        try:
            created_records = WeatherData.objects.bulk_create(all_records_to_insert, ignore_conflicts=True)
            count = len(created_records)
            logger.info(f"[INSTANT INGEST] Stored {count} new unique records for {loc.name_en}.")
            return True # B√°o hi·ªáu th√†nh c√¥ng
        except Exception as e:
            logger.error(f"[INSTANT INGEST] Error bulk inserting weather data for {loc.name_en}: {e}", exc_info=True)
            return False # B√°o hi·ªáu th·∫•t b·∫°i
    
    return not errors_occurred

# @transaction.atomic
# def trigger_data_pruning():
#     """ T√°c v·ª• d·ªçn d·∫πp d·ªØ li·ªáu c≈© - ƒê√£ comment out """
#     logger.info("--- [TASK START] Running Data Pruning ---")
#     ninety_days_ago = timezone.now() - timedelta(days=90)
#     try:
#         deleted_weather, _ = WeatherData.objects.filter(record_time__lt=ninety_days_ago).delete()
#         logger.info(f"[DATA PRUNING] Deleted {deleted_weather} old WeatherData records.")
#
#         thirty_days_ago = timezone.now() - timedelta(days=30)
#         deleted_events, _ = ExtremeEvent.objects.filter(analysis_time__lt=thirty_days_ago).delete()
#         logger.info(f"[DATA PRUNING] Deleted {deleted_events} old ExtremeEvent records.")
#
#         return {'success': True, 'message': f'Deleted {deleted_weather} weather records and {deleted_events} event records.'}
#     except Exception as e:
#         logger.error(f"--- [TASK ERROR] Data Pruning: {e}", exc_info=True)
#         return {'success': False, 'message': 'Error during Data Pruning.'}

