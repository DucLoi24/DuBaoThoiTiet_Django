# api/tasks.py
import requests
import json
import logging
from datetime import datetime, timedelta, date, timezone as dt_timezone # Import timezone tá»« datetime
from django.conf import settings
from django.db import transaction
from django.utils import timezone # Sá»­ dá»¥ng timezone cá»§a Django
from concurrent.futures import ThreadPoolExecutor, as_completed # Import ThreadPoolExecutor

from .models import Location, WeatherData, ExtremeEvent

# Thiáº¿t láº­p logger riÃªng cho file tasks
# Level INFO sáº½ ghi láº¡i cÃ¡c bÆ°á»›c chÃ­nh, DEBUG sáº½ ghi chi tiáº¿t hÆ¡n
logger = logging.getLogger(__name__)

# --- HÃ€M TIá»†N ÃCH CHO TASKS ---

def call_weather_api_from_task(endpoint, params):
    """
    HÃ m gá»i API WeatherAPI dÃ nh riÃªng cho tasks, xá»­ lÃ½ lá»—i chi tiáº¿t hÆ¡n.
    Tráº£ vá» tuple: (data, error_message). data lÃ  None náº¿u cÃ³ lá»—i.
    """
    if not settings.WEATHER_API_KEY:
        logger.error("WEATHER_API_KEY is not configured.")
        return None, "Weather API Key missing"

    params['key'] = settings.WEATHER_API_KEY
    params['lang'] = 'vi'
    full_url = f"{settings.BASE_WEATHER_URL}/{endpoint}.json"

    try:
        # TÄƒng timeout lÃªn 30 giÃ¢y cho cÃ¡c cuá»™c gá»i API máº¡ng
        response = requests.get(full_url, params=params, timeout=30)
        response.raise_for_status() # NÃ©m lá»—i HTTPError cho status >= 400
        return response.json(), None # Tráº£ vá» dá»¯ liá»‡u JSON náº¿u thÃ nh cÃ´ng
    except requests.exceptions.Timeout:
        logger.warning(f"Timeout calling WeatherAPI endpoint: {endpoint} for location: {params.get('q')}")
        return None, "API Timeout"
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code
        try:
            # Cá»‘ gáº¯ng láº¥y lá»—i JSON náº¿u cÃ³
            error_data = e.response.json()
        except json.JSONDecodeError:
            # Náº¿u khÃ´ng pháº£i JSON, láº¥y text thÃ´
            error_data = {'message': e.response.text[:500]} # Giá»›i háº¡n Ä‘á»™ dÃ i lá»—i
        logger.error(f"HTTP Error calling WeatherAPI ({endpoint}) for {params.get('q')}: {status_code} - {error_data}")
        return None, f"API HTTP Error: {status_code}"
    except requests.exceptions.RequestException as e:
        logger.error(f"General Error calling WeatherAPI ({endpoint}) for {params.get('q')}: {e}")
        return None, f"API Request Error: {e}"
    except Exception as e:
        # Ghi láº¡i lá»—i khÃ´ng mong muá»‘n kÃ¨m traceback
        logger.error(f"Unexpected error in call_weather_api_from_task: {e}", exc_info=True)
        return None, f"Unexpected Error: {e}"

def call_local_ai_for_advice(hourly_time_series_data):
    """
    HÃ m gá»i API Ollama cá»¥c bá»™ Ä‘á»ƒ láº¥y lá»i khuyÃªn hoáº·c cáº£nh bÃ¡o THEO YÃŠU Cáº¦U.
    Prompt khÃ¡c biá»‡t: Æ¯u tiÃªn lá»i khuyÃªn, nhÆ°ng sáº½ cáº£nh bÃ¡o náº¿u cÃ³ dáº¥u hiá»‡u cá»±c Ä‘oan.
    Tráº£ vá» dict: {"type": "advice" | "warning", "message_vi": "..."} hoáº·c None náº¿u lá»—i.
    """
    # --- PROMPT Má»šI ---
    prompt = f"""
        **VAI TRÃ’:**
        Báº¡n lÃ  má»™t chuyÃªn gia thá»i tiáº¿t Ä‘á»‹a phÆ°Æ¡ng giÃ u kinh nghiá»‡m táº¡i Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u thá»i tiáº¿t CHI TIáº¾T THEO GIá»œ Ä‘Æ°á»£c cung cáº¥p vÃ  Ä‘Æ°a ra má»™t ÄÃNH GIÃ Tá»”NG QUAN kÃ¨m theo Lá»œI KHUYÃŠN HÃ€NH Äá»˜NG hoáº·c Cáº¢NH BÃO Rá»¦I RO cá»¥ thá»ƒ, há»¯u Ã­ch cho ngÆ°á»i dÃ¹ng trong vÃ i ngÃ y tá»›i.

        **Dá»® LIá»†U Äáº¦U VÃ€O:**
        Dá»¯ liá»‡u thá»i tiáº¿t THEO GIá»œ (-3 Ä‘áº¿n +3 ngÃ y):
        {json.dumps(hourly_time_series_data, indent=2, default=str)}
        # CÃ¡c trÆ°á»ng dá»¯ liá»‡u chÃ­nh: 'time', 'temp_c', 'humidity', 'wind_kph', 'condition_text' (mÃ´ táº£ thá»i tiáº¿t báº±ng chá»¯), 'uv' (chá»‰ sá»‘ UV), 'precip_mm' (lÆ°á»£ng mÆ°a mm), 'chance_of_rain' (tá»· lá»‡ mÆ°a %).

        **QUY TRÃŒNH PHÃ‚N TÃCH VÃ€ SUY LUáº¬N (Báº®T BUá»˜C):**

        **BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh cÃ¡c Hiá»‡n tÆ°á»£ng Ná»•i báº­t (PhÃ¢n tÃ­ch theo tá»«ng yáº¿u tá»‘):**
        * **Nhiá»‡t Ä‘á»™ (`temp_c`):**
            * Náº¯ng nÃ³ng/Oi bá»©c: TÃ¬m cÃ¡c khoáº£ng thá»i gian (Ä‘áº·c biá»‡t ban ngÃ y) `temp_c` > 33Â°C. Ghi nháº­n má»©c Ä‘á»™ (vÃ­ dá»¥: >35Â°C lÃ  nÃ³ng, >38Â°C lÃ  ráº¥t nÃ³ng). Xem xÃ©t káº¿t há»£p `humidity` > 70% Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ oi bá»©c.
            * Láº¡nh/RÃ©t: TÃ¬m cÃ¡c khoáº£ng thá»i gian (Ä‘áº·c biá»‡t Ä‘Ãªm/sÃ¡ng) `temp_c` < 20Â°C. Ghi nháº­n má»©c Ä‘á»™ (vÃ­ dá»¥: <15Â°C lÃ  rÃ©t, <13Â°C lÃ  rÃ©t Ä‘áº­m).
            * Biáº¿n Ä‘á»™ng: Nhiá»‡t Ä‘á»™ cÃ³ thay Ä‘á»•i lá»›n giá»¯a ngÃ y vÃ  Ä‘Ãªm khÃ´ng (> 8-10Â°C)?
        * **Äá»™ áº©m (`humidity`):**
            * áº¨m Æ°á»›t kÃ©o dÃ i: TÃ¬m cÃ¡c khoáº£ng thá»i gian `humidity` > 90% liÃªn tá»¥c nhiá»u giá».
            * KhÃ´ hanh: TÃ¬m cÃ¡c khoáº£ng thá»i gian `humidity` < 40% liÃªn tá»¥c nhiá»u giá» (hiáº¿m á»Ÿ VN nhÆ°ng cáº§n kiá»ƒm tra).
        * **MÆ°a (`precip_mm`, `chance_of_rain`, `condition_text`):**
            * XÃ¡c Ä‘á»‹nh kiá»ƒu mÆ°a: Dá»±a vÃ o `condition_text` (vÃ­ dá»¥: "Light rain" - mÆ°a nhá», "Moderate rain" - mÆ°a vá»«a, "Heavy rain" - mÆ°a to, "Patchy rain possible" - cÃ³ thá»ƒ mÆ°a rÃ o nháº¹, "Thunderstorm" - mÆ°a dÃ´ng).
            * ÄÃ¡nh giÃ¡ cÆ°á»ng Ä‘á»™ vÃ  thá»i gian: Xem `precip_mm` (>1mm/giá» lÃ  Ä‘Ã¡ng ká»ƒ, >5mm/giá» lÃ  mÆ°a khÃ¡ to). Xem `chance_of_rain` (>50% lÃ  kháº£ nÄƒng cao). MÆ°a kÃ©o dÃ i bao lÃ¢u? CÃ³ táº­p trung vÃ o thá»i Ä‘iá»ƒm cá»¥ thá»ƒ (sÃ¡ng, chiá»u, Ä‘Ãªm) khÃ´ng?
        * **GiÃ³ (`wind_kph`):**
            * GiÃ³ máº¡nh/GiÃ³ giáº­t: TÃ¬m cÃ¡c khoáº£ng thá»i gian `wind_kph` > 30 km/h. Ghi nháº­n má»©c Ä‘á»™ (vÃ­ dá»¥: >45 km/h lÃ  giÃ³ máº¡nh, >60 km/h lÃ  ráº¥t máº¡nh). CÃ³ Ä‘i kÃ¨m mÆ°a dÃ´ng khÃ´ng?
        * **Bá»©c xáº¡ UV (`uv`):**
            * Má»©c Ä‘á»™ cao: TÃ¬m cÃ¡c khoáº£ng thá»i gian (thÆ°á»ng 10h-15h) `uv` > 7. Ghi nháº­n má»©c Ä‘á»™ (vÃ­ dá»¥: >9 lÃ  ráº¥t cao). Trá»i cÃ³ náº¯ng (`condition_text` lÃ  "Sunny", "Clear") khÃ´ng?
        * **Hiá»‡n tÆ°á»£ng khÃ¡c (tá»« `condition_text`):**
            * SÆ°Æ¡ng mÃ¹ ("Mist", "Fog")?
            * DÃ´ng, sáº¥m chá»›p ("Thunder")?

        **BÆ°á»›c 2: Tá»•ng há»£p vÃ  XÃ¡c Ä‘á»‹nh Ká»‹ch báº£n ChÃ­nh:**
        * Dá»±a trÃªn cÃ¡c hiá»‡n tÆ°á»£ng ná»•i báº­t Ä‘Ã£ xÃ¡c Ä‘á»‹nh, Ká»ŠCH Báº¢N THá»œI TIáº¾T CHá»¦ Äáº O trong 1-3 ngÃ y tá»›i lÃ  gÃ¬?
            * VÃ­ dá»¥: "Náº¯ng nÃ³ng kÃ¨m oi bá»©c vÃ o ban ngÃ y, chiá»u tá»‘i cÃ³ kháº£ nÄƒng mÆ°a dÃ´ng", "Trá»i nhiá»u mÃ¢y, mÆ°a áº©m kÃ©o dÃ i", "NgÃ y náº¯ng Ä‘Ãªm láº¡nh", "GiÃ³ mÃ¹a Ä‘Ã´ng báº¯c gÃ¢y rÃ©t", "Thá»i tiáº¿t á»•n Ä‘á»‹nh, náº¯ng nháº¹".
        * **Æ¯U TIÃŠN Cáº¢NH BÃO:** Náº¿u cÃ³ báº¥t ká»³ hiá»‡n tÆ°á»£ng nÃ o Ä‘áº¡t ngÆ°á»¡ng nguy hiá»ƒm (náº¯ng nÃ³ng >38Â°C kÃ©o dÃ i, mÆ°a ráº¥t to >10mm/giá», giÃ³ >60 km/h, rÃ©t Ä‘áº­m <13Â°C kÃ©o dÃ i), ká»‹ch báº£n chÃ­nh PHáº¢I LÃ€ Cáº¢NH BÃO vá» hiá»‡n tÆ°á»£ng Ä‘Ã³.

        **BÆ°á»›c 3: ÄÆ°a ra Káº¿t luáº­n vÃ  Lá»i khuyÃªn/Cáº£nh bÃ¡o:**
        * Dá»±a vÃ o Ká»‹ch báº£n ChÃ­nh Ä‘Ã£ xÃ¡c Ä‘á»‹nh:
            * **Náº¿u lÃ  Cáº¢NH BÃO:** PhÃ¡t biá»ƒu rÃµ rÃ ng vá» rá»§i ro chÃ­nh vÃ  Ä‘Æ°a ra lá»i khuyÃªn hÃ nh Ä‘á»™ng cá»¥ thá»ƒ Ä‘á»ƒ phÃ²ng trÃ¡nh. PhÃ¢n loáº¡i lÃ  `"warning"`.
            * **Náº¿u KHÃ”NG cÃ³ cáº£nh bÃ¡o:** PhÃ¡t biá»ƒu vá» ká»‹ch báº£n thá»i tiáº¿t chÃ­nh vÃ  Ä‘Æ°a ra 1-2 lá»i khuyÃªn hÃ nh Ä‘á»™ng thiáº¿t thá»±c liÃªn quan Ä‘áº¿n ká»‹ch báº£n Ä‘Ã³ (vÃ­ dá»¥: trang phá»¥c, hoáº¡t Ä‘á»™ng phÃ¹ há»£p, sá»©c khá»e). PhÃ¢n loáº¡i lÃ  `"advice"`.
        * **YÃªu cáº§u vá» ná»™i dung:**
            * Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu, phÃ¹ há»£p vá»›i ngÆ°á»i Viá»‡t.
            * Lá»i khuyÃªn/cáº£nh bÃ¡o pháº£i cá»¥ thá»ƒ, cÃ³ tÃ­nh hÃ nh Ä‘á»™ng.
            * Ná»™i dung pháº£i logic, nháº¥t quÃ¡n vá»›i dá»¯ liá»‡u phÃ¢n tÃ­ch. **Tuyá»‡t Ä‘á»‘i khÃ´ng bá»‹a Ä‘áº·t hoáº·c Ä‘Æ°a thÃ´ng tin trÃ¡i ngÆ°á»£c dá»¯ liá»‡u (vÃ­ dá»¥: nÃ³i náº¯ng gáº¯t khi dá»¯ liá»‡u bÃ¡o mÆ°a áº©m).**

        **YÃŠU Cáº¦U Äáº¦U RA:**
        Chá»‰ tráº£ lá»i báº±ng ÄÃšNG Má»˜T Ä‘á»‘i tÆ°á»£ng JSON theo cáº¥u trÃºc sau, **KHÃ”NG Sá»¬ Dá»¤NG Láº I CÃC CÃ‚U VÃ Dá»¤ TRONG PHáº¦N MÃ” Táº¢ NÃ€Y**:
        `{{"type": "warning" | "advice", "message_vi": "[Ná»™i dung Ä‘Ã¡nh giÃ¡ tá»•ng quan KÃˆM lá»i khuyÃªn hÃ nh Ä‘á»™ng / cáº£nh bÃ¡o chi tiáº¿t dá»±a trÃªn phÃ¢n tÃ­ch cá»§a báº¡n]"}}`
    """
    # --- Káº¾T THÃšC PROMPT Má»šI ---

    try:
        logger.debug("[LOCAL AI ADVICE] Sending advice request to Ollama...")
        # Timeout cÃ³ thá»ƒ ngáº¯n hÆ¡n cho lá»i khuyÃªn, vÃ­ dá»¥ 2 phÃºt (120 giÃ¢y)
        response = requests.post(settings.OLLAMA_API_URL, json={
            "model": "gemma3:4b", # llama3.1:8b, gemma3:4b
            "prompt": prompt,
            "format": "json",
            "stream": False,
            "keep_alive": "1h"
        }, timeout=300) 
        response.raise_for_status()

        response_data = response.json()
        if 'response' in response_data:
            try:
                # Parse JSON string tá»« response cá»§a Ollama
                result_json = json.loads(response_data['response'])

                # Kiá»ƒm tra cáº¥u trÃºc cÆ¡ báº£n
                if isinstance(result_json, dict) and "type" in result_json and "message_vi" in result_json:
                    logger.info(f"[LOCAL AI ADVICE] Received: {result_json['type']}")
                    return result_json # Tráº£ vá» dict Ä‘Ã£ parse
                else:
                    logger.warning(f"[LOCAL AI ADVICE] Invalid JSON structure received: {result_json}")
                    return None
            except json.JSONDecodeError as e:
                logger.error(f"[LOCAL AI ADVICE] Error parsing JSON response: {e}")
                logger.error(f"Ollama raw response string: {response_data.get('response', 'N/A')}")
                return None
        else:
            logger.warning(f"[LOCAL AI ADVICE] 'response' field missing in Ollama output: {response_data}")
            return None

    except requests.exceptions.Timeout:
        logger.error("[LOCAL AI ADVICE] Timeout calling local Ollama API for advice.")
        return None # Lá»—i timeout tráº£ vá» None
    except requests.exceptions.RequestException as e:
        logger.error(f"[LOCAL AI ADVICE] Error calling Ollama API: {e}")
        return None # Lá»—i káº¿t ná»‘i tráº£ vá» None
    except Exception as e:
        logger.error(f"[LOCAL AI ADVICE] Unexpected error: {e}", exc_info=True)
        return None # Lá»—i khÃ¡c tráº£ vá» None

def call_local_ai_for_analysis(time_series_data):
    """
    HÃ m gá»i API Ollama cá»¥c bá»™ Ä‘á»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u thá»i tiáº¿t.
    Sá»­ dá»¥ng prompt "ChuyÃªn gia Tháº­n trá»ng" vÃ  tráº£ vá» máº£ng cáº£nh bÃ¡o.
    ÄÃ£ cáº­p nháº­t Ä‘á»ƒ xá»­ lÃ½ response linh hoáº¡t (list hoáº·c dict Ä‘Æ¡n).
    """
    prompt = f"""
        **VAI TRÃ’:**
        Báº¡n lÃ  má»™t chuyÃªn gia khÃ­ tÆ°á»£ng thá»§y vÄƒn tháº­n trá»ng.

        **QUY Táº®C VÃ€NG: HÃƒY HOÃ€I NGHI.** CÃ¢u tráº£ lá»i máº·c Ä‘á»‹nh lÃ  má»™t máº£ng rá»—ng [].

        **Dá»® LIá»†U Äáº¦U VÃ€O:**
        Chuá»—i dá»¯ liá»‡u thá»i tiáº¿t 14 ngÃ y (lá»‹ch sá»­ + dá»± bÃ¡o):
        {json.dumps(time_series_data, indent=2, default=str)}

        **CÃC NGÆ¯á» NG KÃCH HOáº T Cáº¢NH BÃO (Chá»‰ bÃ¡o cÃ¡o náº¿u vÆ°á»£t ngÆ°á»¡ng):**
        - ChÃ¡y rá»«ng (INFRASTRUCTURE - HIGH/CRITICAL): Nhiá»‡t Ä‘á»™ (avgtemp_c) > 37Â°C trong ÃT NHáº¤T 3 ngÃ y VÃ€ Ä‘á»™ áº©m (avghumidity) < 40%.
        - Sá»‘c nhiá»‡t (PUBLIC_HEALTH - HIGH): Nhiá»‡t Ä‘á»™ (avgtemp_c) > 38Â°C VÃ€ UV > 10 trong ÃT NHáº¤T 2 ngÃ y.
        - SÃ¢u bá»‡nh (AGRICULTURE - MEDIUM): Äá»™ áº©m (avghumidity) > 90% trong ÃT NHáº¤T 4 ngÃ y VÃ€ nhiá»‡t Ä‘á»™ (avgtemp_c) > 25Â°C.

        **YÃŠU Cáº¦U Äáº¦U RA:**
        Chá»‰ tráº£ lá»i báº±ng má»™t Máº¢NG (array) cÃ¡c Ä‘á»‘i tÆ°á»£ng JSON. Náº¿u khÃ´ng cÃ³ rá»§i ro, tráº£ vá» [].
        Cáº¥u trÃºc cá»§a má»—i Ä‘á»‘i tÆ°á»£ng:
        {{
          "severity": "Má»©c Ä‘á»™ ('MEDIUM', 'HIGH', 'CRITICAL')",
          "impact_field": "LÄ©nh vá»±c ('AGRICULTURE', 'INFRASTRUCTURE', 'PUBLIC_HEALTH')",
          "forecast_details_vi": "MÃ´ táº£ rá»§i ro vÃ  trÃ­ch dáº«n Sá» LIá»†U báº±ng chá»©ng.",
          "actionable_advice_vi": "ÄÆ°a ra má»™t cÃ¢u KHUYáº¾N NGHá»Š hÃ nh Ä‘á»™ng cá»¥ thá»ƒ."
        }}
    """
    try:
        # Log nháº¹ nhÃ ng hÆ¡n khi gá»i AI
        logger.debug("[LOCAL AI] Sending analysis request to Ollama...")
        # TÄƒng timeout lÃªn 5 phÃºt (300 giÃ¢y) vÃ¬ AI cÃ³ thá»ƒ cáº§n nhiá»u thá»i gian
        response = requests.post(settings.OLLAMA_API_URL, json={
            "model": "gemma3:4b",
            "prompt": prompt,
            "format": "json",
            "stream": False,
            "keep_alive": "1h"
        }, timeout=300)
        response.raise_for_status()

        response_data = response.json()
        # Ollama tráº£ vá» JSON string trong trÆ°á»ng 'response'
        if 'response' in response_data:
            try:
                # Parse JSON string tá»« response cá»§a Ollama
                raw_result = json.loads(response_data['response'])

                # --- Xá»­ lÃ½ Response Linh hoáº¡t ---
                if isinstance(raw_result, list):
                    # Náº¿u AI tráº£ vá» Ä‘Ãºng lÃ  má»™t list (ká»ƒ cáº£ list rá»—ng []), dÃ¹ng nÃ³ luÃ´n
                    return raw_result, None
                elif isinstance(raw_result, dict) and raw_result.get('severity', 'NONE').upper() != 'NONE':
                    # Náº¿u AI tráº£ vá» má»™t object vÃ  cÃ³ severity khÃ¡c NONE => cáº£nh bÃ¡o Ä‘Æ¡n láº»
                    logger.warning(f"[LOCAL AI] Ollama returned a single object, wrapping in list: {raw_result}")
                    return [raw_result], None # GÃ³i vÃ o list
                elif isinstance(raw_result, dict) and not raw_result:
                    # Náº¿u AI tráº£ vá» object rá»—ng {}
                    logger.debug("[LOCAL AI] Ollama returned an empty object, treating as no alerts.")
                    return [], None # Coi nhÆ° khÃ´ng cÃ³ cáº£nh bÃ¡o
                else:
                    # CÃ¡c trÆ°á»ng há»£p khÃ¡c (object cÃ³ severity NONE, hoáº·c khÃ´ng pháº£i list/dict)
                    logger.warning(f"[LOCAL AI] Ollama response was not a valid list/object: {raw_result}")
                    return [], "AI response invalid structure"
                # --- Káº¿t thÃºc Xá»­ lÃ½ Linh hoáº¡t ---

            except json.JSONDecodeError as e:
                logger.error(f"[LOCAL AI] Error parsing JSON from Ollama response: {e}")
                logger.error(f"Ollama raw response string: {response_data.get('response', 'N/A')}")
                return [], "AI Response Parsing Error"
        else:
             logger.warning(f"[LOCAL AI] 'response' field missing in Ollama output: {response_data}")
             return [], "AI response field missing"

    except requests.exceptions.Timeout:
        logger.error("[LOCAL AI] Timeout calling local Ollama API (waited 300 seconds).")
        return [], "AI Timeout"
    except requests.exceptions.RequestException as e:
        logger.error(f"[LOCAL AI] Error calling Ollama API: {e}")
        logger.info("ğŸ’¡ Tip: Ensure Ollama is running and the 'gemma3' model is downloaded ('ollama run gemma3').")
        return [], f"AI Connection Error: {e}"
    except Exception as e:
        logger.error(f"[LOCAL AI] Unexpected error during AI analysis: {e}", exc_info=True)
        return [], f"Unexpected AI Error: {e}"

# --- CÃC HÃ€M TÃC Vá»¤ Ná»€N (CRON JOBS) ---

@transaction.atomic # Äáº£m báº£o táº¥t cáº£ cÃ¡c thao tÃ¡c CSDL trong hÃ m nÃ y thÃ nh cÃ´ng hoáº·c tháº¥t báº¡i cÃ¹ng nhau
def trigger_data_ingestion():
    """ 
    TÃ¡c vá»¥ thu tháº­p dá»¯ liá»‡u lá»‹ch sá»­ vÃ  dá»± bÃ¡o (Cron job cháº¡y hÃ ng loáº¡t) 
    HÃ m nÃ y giá» chá»‰ gá»i hÃ m con 'ingest_data_for_single_location'.
    """
    logger.info("--- [TASK START] Running Full Data Ingestion ---")
    active_locations = Location.objects.filter(is_active=True)
    if not active_locations.exists():
        logger.info("[DATA INGESTION] No active locations.")
        return {'success': True, 'message': 'No active locations.'}

    logger.info(f"[DATA INGESTION] Found {active_locations.count()} active location(s).")

    total_success = 0
    total_fail = 0

    # VÃ²ng láº·p nÃ y giá» Ä‘Ã£ sáº¡ch vÃ  Ä‘Æ¡n giáº£n hÆ¡n ráº¥t nhiá»u
    for loc in active_locations:
        try:
            # Gá»i hÃ m con cho tá»«ng Ä‘á»‹a Ä‘iá»ƒm
            success = ingest_data_for_single_location(loc.location_id) 
            if success:
                total_success += 1
            else:
                total_fail += 1
                logger.warning(f"[DATA INGESTION] Failed to ingest data for loc {loc.location_id} during cron job.")
        except Exception as e:
            # Lá»—i nghiÃªm trá»ng khi cháº¡y hÃ m con
            logger.error(f"[DATA INGESTION] Critical error processing loc {loc.location_id}: {e}", exc_info=True)
            total_fail += 1

    errors_occurred = total_fail > 0
    logger.info(f"--- [TASK FINISH] Data Ingestion completed. Succeeded for {total_success} locations. Failed for {total_fail} locations. ---")
    return {'success': not errors_occurred, 'message': f'Data Ingestion completed. Success: {total_success}, Fail: {total_fail}.'}

# --- HÃ€M CON Äá»‚ Xá»¬ LÃ Má»˜T THÃ€NH PHá» (Äá»ŠNH NGHÄ¨A TRÆ¯á»šC) ---
def analyze_single_location(loc):
    """ Láº¥y dá»¯ liá»‡u, gá»i AI vÃ  tráº£ vá» káº¿t quáº£ cho má»™t thÃ nh phá»‘ duy nháº¥t """
    logger.debug(f"[LLM ANALYSIS] Analyzing location: {loc.name_en} (ID: {loc.location_id})")
    # Láº¥y 14 ngÃ y dá»¯ liá»‡u gáº§n nháº¥t, sá»­ dá»¥ng select_related Ä‘á»ƒ tá»‘i Æ°u
    time_series_qs = WeatherData.objects.select_related('location').filter(location=loc).order_by('-record_time')[:14]

    if len(time_series_qs) < 14:
        logger.warning(f"[LLM ANALYSIS] Not enough data for {loc.name_en} ({len(time_series_qs)}/14). Skipping.")
        # Tráº£ vá» lá»—i Ä‘á»ƒ hÃ m cha biáº¿t tÃ¡c vá»¥ con tháº¥t báº¡i
        return loc, [], "Not enough data"

    # Chuyá»ƒn Ä‘á»•i vÃ  sáº¯p xáº¿p cáº©n tháº­n
    try:
        time_series_data = sorted(
            list(time_series_qs.values('record_time', 'temp_c', 'humidity', 'wind_kph', 'data_type')),
            key=lambda x: x['record_time'] # Sáº¯p xáº¿p theo record_time
        )
    except Exception as e:
        logger.error(f"Error preparing time series data for {loc.name_en}: {e}", exc_info=True)
        return loc, [], f"Data preparation error: {e}"

    # Gá»i AI
    alert_results, ai_err = call_local_ai_for_analysis(time_series_data)
    # Tráº£ vá» káº¿t quáº£, bao gá»“m cáº£ lá»—i AI náº¿u cÃ³
    return loc, alert_results, ai_err


# --- HÃ€M PHÃ‚N TÃCH AI CHÃNH (Xá»¬ LÃ Äá»’NG THá»œI) ---
@transaction.atomic # Äáº£m báº£o lÆ°u CSDL an toÃ n khi cháº¡y song song
def trigger_llm_analysis():
    """ TÃ¡c vá»¥ phÃ¢n tÃ­ch AI - Cháº¡y Ä‘á»“ng thá»i cho nhiá»u thÃ nh phá»‘ """
    logger.info("--- [TASK START] Running CONCURRENT Local LLM Analysis ---")
    active_locations = Location.objects.filter(is_active=True)
    if not active_locations.exists():
        logger.info("[LLM ANALYSIS] No active locations.")
        return {'success': True, 'message': 'No active locations.'}

    logger.info(f"[LLM ANALYSIS] Found {active_locations.count()} active location(s) for concurrent analysis.")
    alerts_created_count = 0
    errors_occurred_ai = False # Cá» lá»—i riÃªng cho viá»‡c gá»i/parse AI response
    errors_occurred_db = False # Cá» lá»—i riÃªng cho viá»‡c lÆ°u CSDL
    errors_occurred_data = False # Cá» lá»—i riÃªng cho viá»‡c chuáº©n bá»‹ data

    # Sá»­ dá»¥ng ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=3) as executor: # Giá»›i háº¡n sá»‘ luá»“ng
        # Submit tasks
        future_to_loc_obj = {executor.submit(analyze_single_location, loc_obj): loc_obj for loc_obj in active_locations}

        # Xá»­ lÃ½ káº¿t quáº£ khi hoÃ n thÃ nh
        for future in as_completed(future_to_loc_obj):
            loc_obj_from_future = future_to_loc_obj[future]
            try:
                analyzed_loc_obj, alert_results, task_err = future.result()

                # Kiá»ƒm tra lá»—i tráº£ vá» tá»« tÃ¡c vá»¥ con
                if task_err and task_err != "Not enough data":
                    errors_occurred_ai = True
                    logger.error(f"AI analysis task failed for {analyzed_loc_obj.name_en}: {task_err}")
                    continue
                elif task_err == "Not enough data":
                    errors_occurred_data = True # Ghi nháº­n lá»—i thiáº¿u data
                    continue

                # LÆ°u káº¿t quáº£ cáº£nh bÃ¡o (náº¿u cÃ³)
                if alert_results:
                    logger.info(f"[LLM ANALYSIS] Storing {len(alert_results)} alert(s) for {analyzed_loc_obj.name_en}...")
                    for alert in alert_results:
                        required_keys = ['severity', 'impact_field', 'forecast_details_vi', 'actionable_advice_vi']
                        if isinstance(alert, dict) and all(k in alert and isinstance(alert[k], str) and alert[k] for k in required_keys):
                            try:
                                ExtremeEvent.objects.create(
                                    location=analyzed_loc_obj,
                                    severity=alert['severity'],
                                    impact_field=alert['impact_field'],
                                    forecast_details_vi=alert['forecast_details_vi'],
                                    actionable_advice_vi=alert['actionable_advice_vi'],
                                    raw_llm_json=alert
                                )
                                alerts_created_count += 1
                            except Exception as db_exc:
                                logger.error(f"Error saving alert for {analyzed_loc_obj.name_en}: {db_exc}", exc_info=True)
                                errors_occurred_db = True
                        else:
                            logger.warning(f"[LLM ANALYSIS] Invalid alert structure for {analyzed_loc_obj.name_en}: {alert}")
                            # KhÃ´ng báº­t cá» lá»—i AI á»Ÿ Ä‘Ã¢y náº¿u call_local_ai_for_analysis Ä‘Ã£ xá»­ lÃ½
            except Exception as exc:
                logger.error(f"Error processing result for location {loc_obj_from_future.name_en}: {exc}", exc_info=True)
                errors_occurred_ai = True

    any_critical_errors = errors_occurred_ai or errors_occurred_db
    logger.info(f"--- [TASK FINISH] CONCURRENT LLM Analysis completed. Created {alerts_created_count} alerts. Critical errors: {any_critical_errors} (AI: {errors_occurred_ai}, DB: {errors_occurred_db}, Data: {errors_occurred_data}) ---")
    return {'success': not any_critical_errors, 'message': f'Concurrent analysis completed. Created {alerts_created_count} alerts.'}

def ingest_data_for_single_location(location_id):
    """ TÃ¡c vá»¥ thu tháº­p dá»¯ liá»‡u tá»©c thÃ¬ cho Má»˜T Ä‘á»‹a Ä‘iá»ƒm má»›i. """
    try:
        loc = Location.objects.get(location_id=location_id)
        logger.info(f"[INSTANT INGEST] Running for new location: {loc.name_en} (ID: {loc.location_id})")
    except Location.DoesNotExist:
        logger.error(f"[INSTANT INGEST] Location ID {location_id} not found.")
        return False

    # Láº¥y 7 ngÃ y lá»‹ch sá»­ vÃ  7 ngÃ y dá»± bÃ¡o (giá»‘ng há»‡t logic trong hÃ m cron)
    today = timezone.now().date()
    end_date_hist = today - timedelta(days=1)
    start_date_hist = end_date_hist - timedelta(days=6)
    dt_str = start_date_hist.strftime('%Y-%m-%d')
    end_dt_str = end_date_hist.strftime('%Y-%m-%d')

    all_records_to_insert = []
    errors_occurred = False

    # --- Láº¥y lá»‹ch sá»­ ---
    logger.debug(f"[INSTANT INGEST] Fetching history for {loc.name_en}")
    hist_data, hist_err = call_weather_api_from_task('history', {'q': loc.name_en, 'dt': dt_str, 'end_dt': end_dt_str})
    if hist_data and 'forecast' in hist_data and 'forecastday' in hist_data['forecast']:
        for day in hist_data['forecast']['forecastday']:
            try:
                record_dt_naive = datetime.strptime(day['date'], '%Y-%m-%d')
                record_dt_aware = timezone.make_aware(record_dt_naive, dt_timezone.utc)
                all_records_to_insert.append(WeatherData(
                    location=loc, record_time=record_dt_aware, data_type='HISTORY',
                    temp_c=day['day'].get('avgtemp_c'), humidity=day['day'].get('avghumidity'),
                    uv_index=day['day'].get('uv'), wind_kph=day['day'].get('maxwind_kph'), raw_json=day
                ))
            except (ValueError, KeyError, TypeError) as e:
                logger.warning(f"[INSTANT INGEST] Skipping invalid history record for {loc.name_en} on {day.get('date')}: {e}")
    elif hist_err:
        errors_occurred = True
        logger.error(f"[INSTANT INGEST] Failed to fetch history for {loc.name_en}: {hist_err}")

    # --- Láº¥y dá»± bÃ¡o ---
    logger.debug(f"[INSTANT INGEST] Fetching 7-day forecast for {loc.name_en}")
    fc_data, fc_err = call_weather_api_from_task('forecast', {'q': loc.name_en, 'days': 7})
    if fc_data and 'forecast' in fc_data and 'forecastday' in fc_data['forecast']:
        for day in fc_data['forecast']['forecastday']:
             try:
                record_dt_naive = datetime.strptime(day['date'], '%Y-%m-%d')
                record_dt_aware = timezone.make_aware(record_dt_naive, dt_timezone.utc)
                all_records_to_insert.append(WeatherData(
                    location=loc, record_time=record_dt_aware, data_type='FORECAST',
                    temp_c=day['day'].get('avgtemp_c'), humidity=day['day'].get('avghumidity'),
                    uv_index=day['day'].get('uv'), wind_kph=day['day'].get('maxwind_kph'), raw_json=day
                ))
             except (ValueError, KeyError, TypeError) as e:
                logger.warning(f"[INSTANT INGEST] Skipping invalid forecast record for {loc.name_en} on {day.get('date')}: {e}")
    elif fc_err:
        errors_occurred = True
        logger.error(f"[INSTANT INGEST] Failed to fetch forecast for {loc.name_en}: {fc_err}")

    # --- Bulk insert ---
    if all_records_to_insert:
        try:
            created_records = WeatherData.objects.bulk_create(all_records_to_insert, ignore_conflicts=True)
            count = len(created_records)
            logger.info(f"[INSTANT INGEST] Stored {count} new unique records for {loc.name_en}.")
            return True # BÃ¡o hiá»‡u thÃ nh cÃ´ng
        except Exception as e:
            logger.error(f"[INSTANT INGEST] Error bulk inserting weather data for {loc.name_en}: {e}", exc_info=True)
            return False # BÃ¡o hiá»‡u tháº¥t báº¡i
    
    return not errors_occurred

# @transaction.atomic
# def trigger_data_pruning():
#     """ TÃ¡c vá»¥ dá»n dáº¹p dá»¯ liá»‡u cÅ© - ÄÃ£ comment out """
#     logger.info("--- [TASK START] Running Data Pruning ---")
#     ninety_days_ago = timezone.now() - timedelta(days=90)
#     try:
#         deleted_weather, _ = WeatherData.objects.filter(record_time__lt=ninety_days_ago).delete()
#         logger.info(f"[DATA PRUNING] Deleted {deleted_weather} old WeatherData records.")
#
#         thirty_days_ago = timezone.now() - timedelta(days=30)
#         deleted_events, _ = ExtremeEvent.objects.filter(analysis_time__lt=thirty_days_ago).delete()
#         logger.info(f"[DATA PRUNING] Deleted {deleted_events} old ExtremeEvent records.")
#
#         return {'success': True, 'message': f'Deleted {deleted_weather} weather records and {deleted_events} event records.'}
#     except Exception as e:
#         logger.error(f"--- [TASK ERROR] Data Pruning: {e}", exc_info=True)
#         return {'success': False, 'message': 'Error during Data Pruning.'}

